# -*- coding: utf-8 -*-
"""Code using Fandom

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S7Pazdc0Rak5sTnUrcdfnHz5olHcWb9u
"""

import requests
from bs4 import BeautifulSoup
import time
import csv
import spacy
import pandas as pd

BASE_URL = "https://disney.fandom.com"
START_URL = "https://disney.fandom.com/wiki/Category:Characters"
nlp = spacy.load("en_core_web_sm")

Top_100_URL = "https://www.thewordonpopculture.com/movies/100-greatest-disney-characters"
headers = {
    "User-Agent": "Mozilla/5.0"
}
response = requests.get(Top_100_URL, headers=headers)
soup = BeautifulSoup(response.text, "html.parser")
figures = soup.find_all("figure")

def scrape_top_100(URL):
    """
    Name and pic url from the top 100 Disney characters.
    """

def get_character_links(category_url):
    """
    Gather all character page links from a category page.
    """
    links = []

    response = requests.get(category_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        # On Fandom, character links might be in <li> tags within a "category-page__members" section
        # Adjust selectors as needed
        character_blocks = soup.select("div.category-page__members li")
        for block in character_blocks:
            anchor = block.find("a")
            if anchor:
                links.append(BASE_URL + anchor['href'])
    return links

def build_character_url(name):
    formatted_name = name.replace(" ", "_")
    return f"{BASE_URL}/wiki/{formatted_name}"

def scrape_personality_section(character_url):
    """
    Scrapes the 'Personality' section (if it exists) from a character's page.
    Returns text of that section.
    """
    response = requests.get(character_url)
    personality_text = ""
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")

        # Look for an <h2> or <h3> heading named "Personality" or similar
        headers = soup.find_all(["h2", "h3"])
        for hdr in headers:
            if "Personality" in hdr.get_text():
                # The personality description might be in the sibling <p> tags
                # or in the next few paragraphs until the next heading.
                personality_section = []

                # Move to the next sibling until hitting another heading
                sibling = hdr.find_next_sibling()
                while sibling and sibling.name not in ["h2", "h3"]:
                    if sibling.name == "p":
                        personality_section.append(sibling.get_text(strip=True))
                    sibling = sibling.find_next_sibling()

                personality_text = " ".join(personality_section)
                break  # We found the "Personality" section, so stop scanning

    return personality_text

def extract_personality_adjectives(text):
    doc = nlp(text)

    # Collect adjectives and filter for uniqueness
    adjectives = set()
    for token in doc:
        if token.pos_ == "ADJ":
            adjectives.add(token.lemma_.lower())

    return list(adjectives)

def get_character_image(character_url):
    """
    Extracts the main character image from the Disney Wiki page.
    Returns the image URL if found, otherwise an empty string.
    """
    response = requests.get(character_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")

        # Typical Fandom infobox image is within an <aside> or <figure> inside <a><img>
        infobox = soup.find("aside", class_="portable-infobox")
        if infobox:
            img_tag = infobox.find("img")
            if img_tag and img_tag.has_attr("src"):
                return img_tag["src"]
    return ""

def main():
    # Step 1: Use your list of top 50 character names
    character_names = [
        "Mickey Mouse", "Minnie Mouse", "Donald Duck", "Goofy", "Pluto",
        "Daisy Duck", "Elsa", "Anna", "Olaf", "Kristoff",
        "Simba", "Mufasa", "Nala", "Timon", "Pumbaa",
        "Ariel", "Flounder", "Sebastian", "Belle", "Beast",
        "Gaston", "Cinderella", "Prince Charming", "Snow White", "The Evil Queen",
        "Aurora", "Maleficent", "Aladdin", "Jasmine", "Genie",
        "Abu", "Remy", "Tiana", "Prince Naveen", "Rapunzel",
        "Flynn Rider", "Moana", "Maui", "Stitch", "Lilo",
        "Hercules", "Megara", "Tarzan", "Jane Porter", "Pocahontas",
        "Fa Mulan", "Mushu", "Wreck-It Ralph", "Vanellope von Schweetz", "Buzz Lightyear", "Woody"
    ]

    results_image = []
    results_name = []
    results_personality_text = []
    results_traits = []
    results_link = []

    for name in character_names:
        # Convert name to wiki-safe format
        safe_name = name.replace(" ", "_")
        url = f"{BASE_URL}/wiki/{safe_name}"

        # Try primary URL
        personality_info = scrape_personality_section(url)
        traits = extract_personality_adjectives(personality_info)

        # If personality section is empty, try with "_(character)"
        if not traits:
            fallback_url = f"{BASE_URL}/wiki/{safe_name}_(character)"
            personality_info = scrape_personality_section(fallback_url)
            traits = extract_personality_adjectives(personality_info)
            if traits:
                url = fallback_url  # update link if fallback worked

        image_url = get_character_image(url)
        results_image.append(image_url)

        results_name.append(name)
        results_personality_text.append(personality_info)
        results_traits.append(traits)
        results_link.append(url)

        time.sleep(1)  # be polite with delays

    df = pd.DataFrame({
        "character_name": results_name,
        "personality_text": results_personality_text,
        "traits": results_traits,
        "link": results_link,
        "image": results_image
    })

    return df

main()